{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction convolution\n",
    "\n",
    "[Video de presentation](https://www.loom.com/share/ab551e4390f84636917b363d66085e59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "\n",
    "Expliquer ce qu'est une convolution 2D en traitement d'image et le resultat d'une telle opération sur une image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La convolution 2D en traitement d'image est une opération mathématique qui consiste à appliquer un filtre (aussi appelé noyau ou kernel) sur une image pour en extraire des caractéristiques ou effectuer certaines transformations. Le filtre est une petite matrice de nombres qui se \"déplace\" sur l'image d'entrée. Pour chaque position du filtre, on effectue le produit terme à terme entre le filtre et la portion de l'image qu'il recouvre, puis on somme ces produits pour obtenir une valeur unique. Cette valeur est inscrite dans une nouvelle image de sortie, à la position correspondante du centre du filtre.\n",
    "\n",
    "Résultat d'une telle opération sur une image :\n",
    "\n",
    "Extraction de caractéristiques : La convolution peut être utilisée pour extraire des caractéristiques spécifiques d'une image, comme les bords, les textures ou les motifs. Le type de caractéristique extrait dépend de la nature du filtre utilisé.\n",
    "\n",
    "Filtrage : Les convolutions peuvent appliquer divers types de filtrage sur une image, comme le lissage (réduction du bruit) ou le renforcement des bords (détection des contours).\n",
    "\n",
    "Transformation : Outre le filtrage et l'extraction de caractéristiques, la convolution peut également être utilisée pour appliquer des transformations géométriques ou des effets spéciaux à une image.\n",
    "\n",
    "En résumé, la convolution 2D est un outil puissant en traitement d'image, permettant de modifier ou d'analyser des images de manière significative. Dans le contexte des réseaux de neurones convolutifs (CNN), les convolutions jouent un rôle clé dans l'apprentissage automatique des caractéristiques pertinentes à partir des données visuelles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Couche Convolutionnelle\n",
    "\n",
    "Les réseaux de neurones convolutionnels comportent deux parties : la première qui contient les couches convolutionnelles et la seconde qui est composées de couches Denses. \n",
    "\n",
    "Le code ci-dessous permet de créer une couche convolutionnelle, néanmoins la taille de la convolution est trop grande.\n",
    "\n",
    "De plus, il contient un bug. \n",
    "\n",
    "Modifier le code pour que la taille soit de 3x3 et qu'il fonctionne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = Conv2D(10, kernel_size=(3, 3), activation=\"relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Couche Convolutionnelle - stride\n",
    "\n",
    "Les réseaux de neurones convolutionnels comportent deux parties : la première qui contient les couches convolutionnelles et la seconde qui est composées de couches Denses. \n",
    "\n",
    "Créer une couche convolutionnelle qui contient 30 neurones, un kernel_size de (3, 3) et un strides de 2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = Conv2D(30, kernel_size=(3, 3), strides=(2, 2), activation=\"relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Couche de pooling\n",
    "\n",
    "La partie convolutionnelle d'un réseau peut contenir ce qu'on appelle des couches de Pooling qui permettent de réduire la taille des images. \n",
    "\n",
    "Créer une couche une couche de Max pooling 2D avec comme argument pool_size = (3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pool = MaxPooling2D(pool_size=(3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling\n",
    "\n",
    "En faisant des recherches si besoin, expliquer ce que fait une couche de pooling.\n",
    "\n",
    "Expliquer les avantages et les inconvénients d'utiliser le Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une couche de pooling dans un réseau de neurones convolutionnel (CNN) réduit la dimensionnalité spatiale (largeur et hauteur) des cartes de caractéristiques entrantes, tout en conservant les informations les plus importantes. Le pooling est généralement appliqué après une ou plusieurs couches de convolution.\n",
    "\n",
    "Fonctionnement :\n",
    "\n",
    "Max Pooling : Sélectionne la valeur maximale d'une région spécifiée (par exemple, un carré 2x2) dans la carte de caractéristiques.\n",
    "Average Pooling : Calcule la moyenne de toutes les valeurs dans la région spécifiée de la carte de caractéristiques.\n",
    "Avantages :\n",
    "\n",
    "Réduction de la dimensionnalité : Diminue le nombre de paramètres et de calculs dans le réseau, ce qui réduit le risque de surapprentissage et améliore l'efficacité computationnelle.\n",
    "Invariance à la translation : Aide le modèle à reconnaître des objets dans l'image indépendamment de leur position en conservant les caractéristiques dominantes.\n",
    "Contrôle de l'overfitting : En réduisant la complexité du modèle, le pooling peut aider à prévenir l'overfitting.\n",
    "Inconvénients :\n",
    "\n",
    "Perte d'informations : Le pooling peut entraîner une perte d'informations fines, car il résume les caractéristiques de grandes régions en une seule valeur (max ou moyenne).\n",
    "Moins de précision spatiale : En réduisant la résolution spatiale, il peut être plus difficile pour le réseau de détecter des objets de petite taille ou des détails fins.\n",
    "Choix fixe : Le choix du type de pooling (max ou average) et de la taille de la région de pooling est fixe et peut ne pas être optimal pour toutes les applications ou toutes les parties de l'image.\n",
    "En résumé, les couches de pooling sont un composant essentiel des CNNs pour réduire la complexité et améliorer l'efficacité, mais elles doivent être utilisées judicieusement pour équilibrer entre la réduction de dimensionnalité et la conservation des informations importantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesser la data\n",
    "Les couches convolutionnelles 2D  on besoin de Tenseur 4D ! \n",
    "\n",
    "La première dimension représente toujours le nombre d'exemples\n",
    "\n",
    "La seconde et la troisième représentent les dimensions de l'image\n",
    "\n",
    "La quatrième représente le nombre de cannaux (1 pour noir et blanc, 3 pour les couleurs)\n",
    "\n",
    "Il y a quatre tableaux numpy chargés dans votre environnement:\n",
    "\n",
    "x_train et x_test (images) et y_train et y_test (labels correspondants aux chiffres affichées par les images)\n",
    "\n",
    "1. Afficher la taille de x_train et x_test\n",
    "\n",
    "2. Avec la méthode reshape, modifier la taille de x_train et x_test pour que toutes les images passe d'une taille (28, 28) à (28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de x_train après redimensionnement: (200, 28, 28, 1)\n",
      "Taille de x_test après redimensionnement: (30, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Modifier directement la taille de x_train et x_test\n",
    "import numpy as np\n",
    "\n",
    "# Créer un tableau x_test de taille (30, 28, 28)\n",
    "x_test = np.random.rand(30, 28, 28)\n",
    "\n",
    "# Créer un tableau x_train de taille (200, 28, 28)\n",
    "x_train = np.random.rand(200, 28, 28)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# Afficher les tailles après modification pour vérification\n",
    "print(\"Taille de x_train après redimensionnement:\", x_train.shape)\n",
    "print(\"Taille de x_test après redimensionnement:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Créer le modèle CNN\n",
    "\n",
    "Avec Keras créer un réseau de neurones avec les couches suivantes : \n",
    "\n",
    "Convolution avec filters=8 et kernel_size=(3, 3)\n",
    "Flatten\n",
    "Dense avec 10 neurones et une  activation softmax\n",
    "Spécifier l'argument input_shape en supposant qu'on a des images en noir et blanc de taille 28 par 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nathan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5408</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">54,090</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │            \u001b[38;5;34m80\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5408\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │        \u001b[38;5;34m54,090\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">54,170</span> (211.60 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m54,170\u001b[0m (211.60 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">54,170</span> (211.60 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m54,170\u001b[0m (211.60 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Flatten, Dense\n",
    "\n",
    "# Création du modèle CNN\n",
    "model = Sequential([\n",
    "    # Couche de convolution\n",
    "    Conv2D(filters=8, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    # Aplatir les données pour la couche dense\n",
    "    Flatten(),\n",
    "    # Couche dense avec 10 neurones et activation softmax\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Afficher le résumé du modèle\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiler le modèle\n",
    "\n",
    "On a créé un réseau de neurone convolutionnel qui est stocké dans la variable   model\n",
    "\n",
    "Compilez le avec l'optimiseur Adam, la fonction de coût sparse_categorical_crossentropy et la métrique accuracy. Il faudra mettre la métrique accuracy dans une liste (en effet Keras peut accepter plusieurs métriques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiler le modèle\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraîner le modèle\n",
    "\n",
    "On a chargé un CNN dans la variable model et on l'a compilé.\n",
    "\n",
    "Les variables model, x_train, x_test, y_train et y_test sont toutes chargées dans votre environnement.\n",
    "\n",
    "Entrainez le modèle avec 6 époques (epoch). \n",
    "\n",
    "\n",
    "Keras permet également de mesurer régulièrement les performances du modèle sur un jeu de test au cours de l'entraînement. Faire ensorte que ça soit le cas. Pour cela vous devrez utiliser l'argument validation_data (cf : https://www.tensorflow.org/api_docs/python/tf/keras/Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(None, 10), output.shape=(None, 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m y_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Entraîner le modèle\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py:616\u001b[0m, in \u001b[0;36msparse_categorical_crossentropy\u001b[1;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[0;32m    610\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    611\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `output` must be at least rank 1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    612\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    613\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    614\u001b[0m     )\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    617\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `output` must have rank (ndim) `target.ndim - 1`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    618\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    619\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    620\u001b[0m     )\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n",
      "\u001b[1;31mValueError\u001b[0m: Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(None, 10), output.shape=(None, 10)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_train = np.random.rand(200, 10)\n",
    "y_test = np.random.rand(30, 10)\n",
    "\n",
    "# Entraîner le modèle\n",
    "history = model.fit(x_train, y_train, epochs=6, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Différence couche Dense et Convolutionnelle\n",
    "\n",
    "Expliquer la différence entre une couche Dense et convolutionnelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GitHub Copilot\n",
    "Les couches Dense et convolutionnelles sont deux types de couches utilisées dans les réseaux de neurones, mais elles servent des objectifs différents et fonctionnent de manière distincte :\n",
    "\n",
    "Couche Dense (Fully Connected) : Dans une couche dense, chaque neurone est connecté à tous les neurones de la couche précédente. Cela signifie que l'information de l'entrée est traitée globalement. Les couches denses sont souvent utilisées pour apprendre des patterns non spatiaux dans les données ou pour la classification finale dans un réseau de neurones. Elles nécessitent que les données d'entrée soient de forme vectorielle (1D).\n",
    "\n",
    "Couche Convolutionnelle : Les couches convolutionnelles appliquent un filtre (ou noyau) sur l'entrée pour créer une carte des caractéristiques (feature map), en préservant la structure spatiale des données. Cela les rend particulièrement adaptées pour traiter des données avec une forte corrélation spatiale, comme les images. Les couches convolutionnelles permettent de détecter des caractéristiques locales (comme les bords dans une image) en appliquant le même filtre sur différentes parties de l'entrée, ce qui les rend également efficaces en termes de nombre de paramètres à apprendre.\n",
    "\n",
    "En résumé, la principale différence réside dans le fait que les couches convolutionnelles traitent les données de manière locale et spatiale, ce qui est idéal pour les données d'image, tandis que les couches denses traitent les données de manière globale, sans tenir compte de la structure spatiale, ce qui les rend plus adaptées pour la classification ou pour traiter des données déjà transformées en vecteurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Todo) Viz des métriques sur donnéees d'entrainement et de validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
